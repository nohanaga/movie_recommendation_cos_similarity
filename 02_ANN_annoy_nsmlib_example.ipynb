{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light FM 近似最近傍探索 Approximate Nearest Neighbours (ANN)\n",
    "\n",
    "このノートブックでは、例の movielens データセット上にハイブリッド LightFM モデルを構築します。[LightFM](https://github.com/lyst/lightfm)パッケージを使用して純粋な行列分解とハイブリッドモデルを構築する手順を概説しています。また、適合したハイブリッドモデルからユーザーとアイテムの類似性の両方を抽出する方法も示しています。より詳細な LightFM モデルの構築は、[Microsoft Recommenders](https://github.com/microsoft/recommenders/blob/master/examples/02_model_hybrid/lightfm_deep_dive.ipynb) を参照してください。\n",
    "\n",
    "次に、アイテムとユーザのベクトルを格納し、スケールでの高速なクエリのために、これらを対象とした ANN インデックスを構築します。この手法では、何百万ものアイテムに対して 50 ミリ秒以下の応答時間を実現できます。\n",
    "\n",
    "2 つの人気のある ANN ライブラリ、Annoy と NMSlib を見て、その性能を比較します。\n",
    "\n",
    "依存関係:\n",
    "- Annoy - https://github.com/spotify/annoy\n",
    "- NMSLIB - https://github.com/searchivarius/nmslib/tree/master/python_bindings\n",
    "\n",
    "まずはデータをインポートして、ドキュメントの例のように Light FM モデルを構築してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightfm ライブラリのインストール\n",
    "! pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n",
    "from lightfm.datasets import fetch_movielens\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "movielens = fetch_movielens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in movielens.items():\n",
    "    print(key, type(value), value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = movielens['train']\n",
    "test = movielens['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "model = LightFM(learning_rate=0.05, loss='warp', no_components=64, item_alpha=0.001)\n",
    "\n",
    "model.fit_partial(train, item_features=movielens['item_features'], epochs=20 )\n",
    "\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "項目の特徴量にモデルの項目特徴量を乗算するだけで、項目のエンベッディングを得ることができます。さらに良いことに、LightFM には、特徴量の集合を与えて、これらのエンベッディングを取得するための機能が組み込まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, item_embeddings = model.get_item_representations(movielens['item_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、項目間の問い合わせのための Annoy インデックスを作ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "factors = item_embeddings.shape[1] #インデックス化される項目ベクトルの長さ\n",
    "annoy_idx = AnnoyIndex(factors)  \n",
    "for i in range(item_embeddings.shape[0]):\n",
    "    v = item_embeddings[i]\n",
    "    annoy_idx.add_item(i, v)\n",
    "\n",
    "annoy_idx.build(10) # 10 trees\n",
    "annoy_idx.save('movielens_item_Annoy_idx.ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、類似したムービーのインデックスを検索します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_movies_annoy(movie_id, index, n=10, print_output=True):\n",
    "    nn = index.get_nns_by_item(movie_id, 10)\n",
    "    if print_output:\n",
    "        print('Closest to %s : \\n' % movielens['item_labels'][movie_id])\n",
    "    titles = [movielens['item_labels'][i] for i in nn]\n",
    "    if print_output:\n",
    "        print(\"\\n\".join(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_movies_annoy(90, annoy_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "悪くはない結果です。そして高速な処理速度が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "nearest_movies_annoy(90, annoy_idx, print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMSlib はどうでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "# コサイン類似度のHNSWインデックスを用いて、新しいnmslibインデックスを初期化します。\n",
    "nms_idx = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "nms_idx.addDataPointBatch(item_embeddings)\n",
    "nms_idx.createIndex(print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_movies_nms(movie_id, index, n=10, print_output=True):\n",
    "    nn = index.knnQuery(item_embeddings[movie_id], k=10)\n",
    "    if print_output == True:\n",
    "        print('Closest to %s : \\n' % movielens['item_labels'][movie_id])\n",
    "    titles = [movielens['item_labels'][i] for i in nn[0]]\n",
    "    if print_output == True:\n",
    "        print(\"\\n\".join(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_movies_nms(90, nms_idx, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "nearest_movies_nms(90, nms_idx, n=10, print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またしても、良い、おそらく少し良くなっている、また非常に高速です。さて、Xbox のレコメンデーション チームによって概説された巧妙なトリックを使用して、ユーザーのレコメンデーションを行う方法を示す例を紹介します: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/XboxInnerProduct.pdf. Ben Fredrickson https://github.com/benfred/ に感謝します。\n",
    "\n",
    "基本的には、各項目ベクトルに正規化係数を追加して、その距離を互いに等しくします。そして、ユーザ・ベクトルで問い合わせを行うとき、最後に 0 を追加し、結果はユーザ・ベクトルとアイテム・ベクトルの内積に比例します。これは，近似最大内積検索を行う COOL な方法です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(item_embeddings, axis=1)\n",
    "max_norm = norms.max()\n",
    "extra_dimension = np.sqrt(max_norm ** 2 - norms ** 2)\n",
    "norm_data = np.append(item_embeddings, extra_dimension.reshape(norms.shape[0], 1), axis=1)\n",
    "\n",
    "#First an Annoy index:\n",
    "\n",
    "user_factors = norm_data.shape[1]\n",
    "annoy_member_idx = AnnoyIndex(user_factors)  # Length of item vector that will be indexed\n",
    "\n",
    "for i in range(norm_data.shape[0]):\n",
    "    v = norm_data[i]\n",
    "    annoy_member_idx.add_item(i, v)\n",
    "    \n",
    "annoy_member_idx.build(10)\n",
    "\n",
    "# Now an NMS index\n",
    "\n",
    "nms_member_idx = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "nms_member_idx.addDataPointBatch(norm_data)\n",
    "nms_member_idx.createIndex(print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーザーベクトルを定義する\n",
    "\n",
    "_, user_embeddings = model.get_user_representations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### これで、標準のLightFMの例のようにクエリを実行することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recommendation(user_ids, model, data, n_items=10, print_output=True):\n",
    "    n_users, n_items = data['train'].shape\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        top_items = [data['item_labels'][i] for i in annoy_member_idx.get_nns_by_vector(np.append(user_embeddings[user_id], 0), 50)]\n",
    "        if print_output == True:\n",
    "            print(\"User %s\" % user_id)\n",
    "            print(\"     Known positives:\")\n",
    "\n",
    "            for x in known_positives[:3]:\n",
    "                print(\"        %s\" % x)\n",
    "\n",
    "            print(\"     Recommended:\")\n",
    "\n",
    "            for x in top_items[:3]:\n",
    "                print(\"        %s\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recommendation([3,25,450], model, movielens, print_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ここでオリジナルの非 ANN 版と比較してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recommendation_original(model, data, user_ids, print_output=True):\n",
    "\n",
    "    n_users, n_items = data['train'].shape\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        known_positives = data['item_labels'][data['train'].tocsr()[user_id].indices]\n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        top_items = data['item_labels'][np.argsort(-scores)]\n",
    "        if print_output == True:\n",
    "            print(\"User %s\" % user_id)\n",
    "            print(\"     Known positives:\")\n",
    "\n",
    "            for x in known_positives[:3]:\n",
    "                print(\"        %s\" % x)\n",
    "\n",
    "            print(\"     Recommended:\")\n",
    "\n",
    "            for x in top_items[:3]:\n",
    "                print(\"        %s\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recommendation_original(model, movielens, [3, 25, 450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "かなり似ていますね！\n",
    "\n",
    "## どのくらいの速さでスケールするのでしょうか？\n",
    "\n",
    "ここでは、1 つの予測で比較した 2 つの機能を紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sample_recommendation_original(model, movielens, [3, 25, 450], print_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sample_recommendation([3,25,450], model, movielens, print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのため、これは少し速くなりますが、これらのボリュームでは、おそらく余分な努力を正当化するものではないでしょう。これらのライブラリが価値を発揮するのは、非常に大量のベクトルに対してです。では、ANN ライブラリと Scikit-Learn のペアワイズ距離関数との比較を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_nearest_neighbours_methods(item_embeddings):\n",
    "    \n",
    "    # Pairwise distances nearest neighbours\n",
    "    start = time.perf_counter()\n",
    "    distances = pairwise_distances(item_embeddings[0].reshape(1, -1), Y=item_embeddings)\n",
    "    top_items = distances.argsort()[:10]\n",
    "    end = time.perf_counter()\n",
    "    pairwise_elapsed = end - start\n",
    "    \n",
    "    #NMS lib\n",
    "    nms_idx = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    nms_idx.addDataPointBatch(item_embeddings)\n",
    "    nms_idx.createIndex(print_progress=False)\n",
    "    start = time.perf_counter()\n",
    "    nms_idx.knnQuery(item_embeddings[0])[0]\n",
    "    end =time.perf_counter()\n",
    "    nms_elapsed = end - start\n",
    "\n",
    "    #Annoy\n",
    "    annoy_idx = AnnoyIndex(item_embeddings.shape[1])\n",
    "    for i in range(item_embeddings.shape[0]):\n",
    "        v = item_embeddings[i]\n",
    "        annoy_idx.add_item(i, v)\n",
    "    annoy_idx.build(10) # 10 trees\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    annoy_idx.get_nns_by_vector(item_embeddings[0], 10)\n",
    "    end = time.perf_counter()    \n",
    "    annoy_elapsed =  end - start\n",
    "   \n",
    "    return {'pairwise_elapsed': pairwise_elapsed, 'nms_elapsed': nms_elapsed, 'annoy_elapsed' : annoy_elapsed}\n",
    "\n",
    "pairwise_timings = []\n",
    "nms_timings = []\n",
    "annoy_timings = []\n",
    "\n",
    "for i in range (1, 100, 10):\n",
    "    loop_item_embeddings = np.repeat(item_embeddings, i, axis=0)\n",
    "    results = time_nearest_neighbours_methods(loop_item_embeddings)\n",
    "    pairwise_timings.append(results['pairwise_elapsed'])\n",
    "    nms_timings.append(results['nms_elapsed'])\n",
    "    annoy_timings.append(results['annoy_elapsed'])\n",
    "    \n",
    "timings_df = pd.DataFrame({'Pairwise_dist':pairwise_timings, \\\n",
    "                           'NMS': nms_timings, \\\n",
    "                           'Annoy': annoy_timings}, \\\n",
    "                          index=np.arange(10)*item_embeddings.shape[0])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "ax = timings_df.plot(figsize=(14,4), title='Time to compute nearest neighbours vs. number of vectors', ax=axes[0])\n",
    "ax.set(xlabel='Numer of vectors', ylabel='Time (ms)')\n",
    "\n",
    "ax2 = timings_df.loc[:, 'NMS':'Annoy'].plot(figsize=(14,4), title='Time to compute nearest neighbours vs. number of vectors', ax=axes[1])\n",
    "ax2.set(xlabel='Numer of vectors', ylabel='Time (ms)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ご覧のように、NMSLib と Annoy の両方とも、より多くのベクタ数でペアワイズ距離法よりもはるかに高速です。実際、これらのボリュームでは、ベクタ数を増やしても応答時間の増加は全く見られません。\n",
    "\n",
    "## では、ANNライブラリの使用は品質にどのように影響するのでしょうか？\n",
    "\n",
    "アイデアを得るために、Annoy と NMS インデックスを使って予測値を計算し、LightFM のビルトイン予測メソッドと比較して、それらの precision@k を評価してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightfm_pak = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "def annoy_precision_at_k(user_ids, k=10):\n",
    "    test_csr = test.tocsr()\n",
    "    paks = []\n",
    "    for user_id in user_ids:\n",
    "        test_interactions = test_csr[user_id].indices\n",
    "        if len(test_interactions) > 0:\n",
    "            recommendations = annoy_member_idx.get_nns_by_vector(np.append(user_embeddings[user_id], 0), k)\n",
    "            hits = len(set(test_interactions).intersection(recommendations))\n",
    "            pak = hits / (len(test_interactions) * 1.0)\n",
    "            paks.append(pak)\n",
    "    return np.array(paks).mean()\n",
    "\n",
    "annoy_pak = annoy_precision_at_k(np.arange(test.shape[0]))\n",
    "\n",
    "def nms_precision_at_k(user_ids, k=10):\n",
    "    test_csr = test.tocsr()\n",
    "    paks = []\n",
    "    for user_id in user_ids:\n",
    "        test_interactions = test_csr[user_id].indices\n",
    "        if len(test_interactions) > 0:\n",
    "            recommendations = nms_member_idx.knnQuery(np.append(user_embeddings[user_id], 0), k=10)[0]\n",
    "            hits = len(set(test_interactions).intersection(recommendations))\n",
    "            pak = hits / (len(test_interactions) * 1.0)\n",
    "            paks.append(pak)\n",
    "    return np.array(paks).mean()\n",
    "\n",
    "nms_pak = nms_precision_at_k(np.arange(test.shape[0]))\n",
    "\n",
    "plt.bar(['LightFm', 'Annoy', 'NMSLib'], [lightfm_pak, annoy_pak, nms_pak])\n",
    "plt.title('Precision at K=10, ANN vs. exact')\n",
    "plt.ylabel('Precision at k=10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## では、これらのライブラリのどれを使うべきでしょうか？\n",
    "\n",
    "小規模なデータセットでは、わずかな速度向上のために手間をかけたり精度を落としたりする価値はありません。しかし、100ms 以下で応答する必要がある場合は、これらのライブラリのいずれかを検討する必要がある前に、ベクトルセットが大きくなるだけです。\n",
    "\n",
    "Annoy はメモリマップに保存できるという利点があるので、本番環境では複数のプロセスでも問題なく動作します。\n",
    "\n",
    "NMSlib は精度の面では Annoy を凌駕していますが、大規模なデータセットのインデックス構築には時間がかかることもわかりました。これは、動きの速い製品カタログなどのために頻繁にインデックスを再構築しなければならない場合に問題になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
